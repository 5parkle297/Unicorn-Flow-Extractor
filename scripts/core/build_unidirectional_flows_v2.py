"""
å•å‘æµæ„å»ºå™¨ V2 - æ•´åˆ NFStream å’Œ NetMamba çš„æ ¸å¿ƒæ€è·¯

æ ¸å¿ƒæ”¹è¿›:
1. IP Masking - é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šIP
2. Header + Payload åˆ†ç¦» - ä¿ç•™åè®®ä¿¡æ¯
3. Byte Balancing - å›ºå®šé•¿åº¦è¾“å…¥
4. åŒ…æ•°é‡é™åˆ¶ - æ¯æµæœ€å¤šNä¸ªåŒ…
5. æ™ºèƒ½è¿‡æ»¤ - å¯é€‰è¿‡æ»¤æ— payloadåŒ…
6. å¤šç§è¾“å‡ºæ ¼å¼ - å…¼å®¹ä¸åŒä¸‹æ¸¸ä»»åŠ¡

æ”¯æŒä¸¤ç§è¾“å…¥æ¨¡å¼:
1. CSVæ¨¡å¼ - ä½¿ç”¨tsharkå¯¼å‡ºçš„CSVï¼ˆå…¼å®¹ç°æœ‰æµç¨‹ï¼‰
2. PCAPæ¨¡å¼ - ç›´æ¥è§£æPCAPæ–‡ä»¶ï¼ˆNetMambaé£æ ¼ï¼Œæ›´å®Œæ•´ï¼‰

ä½œè€…: åŸºäºNetMambaå’ŒNFStreamæ€è·¯æ•´åˆ
"""

import argparse
import csv
import pickle
import numpy as np
import binascii
from collections import defaultdict
from pathlib import Path
import sys

# å¢åŠ  CSV å­—æ®µå¤§å°é™åˆ¶
csv.field_size_limit(100 * 1024 * 1024)

# ==================== é…ç½®å‚æ•° ====================

DEFAULT_CONFIG = {
    # åŒ…æ•°é‡é™åˆ¶
    'max_packets_per_flow': 100,     # æ¯æµæœ€å¤§åŒ…æ•°ï¼ˆNFStreamé£æ ¼ï¼‰
    'netmamba_packets': 5,           # NetMambaä½¿ç”¨5ä¸ªåŒ…

    # Headerå’ŒPayloadé•¿åº¦ï¼ˆå­—èŠ‚ï¼‰
    'header_bytes': 80,              # NetMamba: 80å­—èŠ‚header
    'payload_bytes': 240,            # NetMamba: 240å­—èŠ‚payload

    # IPå¤„ç†
    'ip_masking': True,              # æ˜¯å¦è¿›è¡ŒIP Masking
    'mask_ip': "0.0.0.0",            # æ©ç åçš„IPåœ°å€

    # è¿‡æ»¤ç­–ç•¥
    'filter_empty_payload': False,   # æ˜¯å¦è¿‡æ»¤æ— payloadçš„åŒ…

    # ğŸ†• æµè¶…æ—¶é…ç½®
    'flow_timeout': 120,             # æµè¶…æ—¶æ—¶é—´(ç§’),120ç§’æ— æ´»åŠ¨åˆ™åˆ†å‰²æ–°æµ
    'enable_timeout': True,          # æ˜¯å¦å¯ç”¨è¶…æ—¶æœºåˆ¶

    # è¾“å‡ºæ ¼å¼
    'output_format': 'dict',         # dict, netmamba, sequence
}


# ==================== å·¥å…·å‡½æ•° ====================

def safe_int(x):
    """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
    try:
        return int(x)
    except:
        return None


def safe_float(x):
    """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
    try:
        return float(x)
    except:
        return None


def hex_to_bytes(hex_str):
    """åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬å­—èŠ‚æ•°ç»„"""
    if not hex_str:
        return np.array([], dtype=np.uint8)
    try:
        return np.array([int(hex_str[i:i+2], 16) for i in range(0, len(hex_str), 2)], dtype=np.uint8)
    except:
        return np.array([], dtype=np.uint8)


def bytes_to_hex(byte_array):
    """å­—èŠ‚æ•°ç»„è½¬åå…­è¿›åˆ¶å­—ç¬¦ä¸²"""
    return ''.join(f'{b:02x}' for b in byte_array)


def balance_bytes(data, target_length, pad_value=0):
    """
    Byte Balancing: å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
    
    Args:
        data: å­—èŠ‚æ•°ç»„æˆ–åå…­è¿›åˆ¶å­—ç¬¦ä¸²
        target_length: ç›®æ ‡å­—èŠ‚é•¿åº¦
        pad_value: å¡«å……å€¼ï¼ˆé»˜è®¤0ï¼‰
    
    Returns:
        å›ºå®šé•¿åº¦çš„numpyæ•°ç»„
    """
    if isinstance(data, str):
        # åå…­è¿›åˆ¶å­—ç¬¦ä¸²
        hex_len = target_length * 2
        if len(data) > hex_len:
            data = data[:hex_len]
        else:
            data = data + '0' * (hex_len - len(data))
        return hex_to_bytes(data)
    else:
        # å­—èŠ‚æ•°ç»„
        data = np.array(data, dtype=np.uint8)
        if len(data) > target_length:
            return data[:target_length]
        else:
            padded = np.full(target_length, pad_value, dtype=np.uint8)
            padded[:len(data)] = data
            return padded


def determine_direction(ip1, ip2):
    """ç¡®å®šæµæ–¹å‘ï¼ˆåŸºäºIPå­—å…¸åºï¼‰"""
    return 0 if str(ip1) < str(ip2) else 1


def should_split_flow(packets, new_pkt_ts, timeout=120):
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦å› è¶…æ—¶åˆ†å‰²æµ

    Args:
        packets: å½“å‰æµçš„åŒ…åˆ—è¡¨
        new_pkt_ts: æ–°åŒ…çš„æ—¶é—´æˆ³
        timeout: è¶…æ—¶æ—¶é—´(ç§’)

    Returns:
        bool: Trueè¡¨ç¤ºéœ€è¦åˆ†å‰²
    """
    if not packets:
        return False

    last_pkt = packets[-1]
    return (new_pkt_ts - last_pkt['ts']) > timeout


# ==================== CSVæ¨¡å¼å¤„ç† ====================

class CSVFlowBuilder:
    """åŸºäºtshark CSVçš„æµæ„å»ºå™¨ï¼ˆå…¼å®¹ç°æœ‰æµç¨‹ï¼‰"""
    
    def __init__(self, config=None):
        self.config = {**DEFAULT_CONFIG, **(config or {})}
    
    def build_flows(self, csv_file):
        """
        ä»tshark CSVæ„å»ºå•å‘æµ

        Args:
            csv_file: tsharkå¯¼å‡ºçš„CSVæ–‡ä»¶è·¯å¾„

        Returns:
            flows: dict, æµå­—å…¸
            stats: dict, ç»Ÿè®¡ä¿¡æ¯
        """
        flows = defaultdict(list)
        flow_counters = defaultdict(int)  # ğŸ†• è®°å½•æ¯ä¸ªåŸºç¡€æµçš„åºå·
        stats = {
            'total_packets': 0,
            'valid_packets': 0,
            'skipped_empty_payload': 0,
            'skipped_invalid': 0,
            'flows_split_by_timeout': 0  # ğŸ†• å› è¶…æ—¶åˆ†å‰²çš„æµæ•°é‡
        }
        
        # å°è¯•ä¸åŒç¼–ç 
        encodings = ['utf-16-le', 'utf-8', 'latin-1']
        
        for encoding in encodings:
            try:
                with open(csv_file, "r", newline='', encoding=encoding) as f:
                    reader = csv.DictReader(f)
                    
                    for row in reader:
                        stats['total_packets'] += 1
                        pkt = self._process_packet(row)
                        
                        if pkt is None:
                            stats['skipped_invalid'] += 1
                            continue
                        
                        # å¯é€‰ï¼šè¿‡æ»¤æ— payloadçš„åŒ…
                        if self.config['filter_empty_payload'] and not pkt['payload']:
                            stats['skipped_empty_payload'] += 1
                            continue

                        # ğŸ†• æ£€æŸ¥æµè¶…æ—¶
                        if self.config['enable_timeout']:
                            base_key = pkt['flow_key'][:5]  # ä¸å«directionçš„äº”å…ƒç»„

                            # æ£€æŸ¥æ˜¯å¦éœ€è¦å› è¶…æ—¶åˆ†å‰²æµ
                            if pkt['flow_key'] in flows and flows[pkt['flow_key']]:
                                if should_split_flow(flows[pkt['flow_key']], pkt['ts'],
                                                    self.config['flow_timeout']):
                                    # åˆ›å»ºæ–°çš„æµkey (æ·»åŠ æµåºå·)
                                    flow_counters[base_key] += 1
                                    # æ–°keyæ ¼å¼: (ip1, ip2, port1, port2, proto, direction, seq)
                                    new_key = (*pkt['flow_key'], flow_counters[base_key])
                                    pkt['flow_key'] = new_key
                                    stats['flows_split_by_timeout'] += 1

                        stats['valid_packets'] += 1
                        flows[pkt['flow_key']].append(pkt)
                    
                    break  # æˆåŠŸè¯»å–ï¼Œé€€å‡ºå¾ªç¯
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"[!] Error reading CSV with {encoding}: {e}")
                continue
        
        # åå¤„ç†
        flows = self._post_process_flows(dict(flows))
        
        return flows, stats
    
    def _process_packet(self, row):
        """å¤„ç†å•ä¸ªåŒ…"""
        # æå–åŸºç¡€å­—æ®µ
        src = row.get("ip.src", "")
        dst = row.get("ip.dst", "")
        
        sport = safe_int(row.get("tcp.srcport")) or safe_int(row.get("udp.srcport"))
        dport = safe_int(row.get("tcp.dstport")) or safe_int(row.get("udp.dstport"))
        
        if not src or not dst or sport is None or dport is None:
            return None
        
        # IP Maskingï¼ˆåœ¨æµkeyä¸­ä½¿ç”¨åŸå§‹IPï¼Œä½†å¯ä»¥åœ¨ç‰¹å¾ä¸­ä½¿ç”¨maskï¼‰
        original_src, original_dst = src, dst
        if self.config['ip_masking']:
            # å¯¹äºæµåˆ†ç»„ï¼Œä»ä½¿ç”¨åŸå§‹IP
            # ä½†åœ¨åç»­ç‰¹å¾æå–æ—¶å¯ä»¥mask
            pass
        
        # åè®®åˆ¤æ–­
        if safe_int(row.get("tcp.srcport")) is not None:
            protocol = 6  # TCP
        elif safe_int(row.get("udp.srcport")) is not None:
            protocol = 17  # UDP
        else:
            protocol = -1
        
        # æ–¹å‘å’Œæµkey
        direction = determine_direction(src, dst)
        if direction == 0:
            flow_key = (src, dst, sport, dport, protocol, 0)
        else:
            flow_key = (dst, src, dport, sport, protocol, 1)
        
        # æ—¶é—´æˆ³å’Œé•¿åº¦
        ts = safe_float(row.get("frame.time_epoch"))
        if ts is None:
            return None
        
        length = safe_int(row.get("frame.len")) or 0
        
        # Payload
        payload_hex = row.get("data.data", "") or ""
        # æ¸…ç†payloadï¼ˆå»é™¤å¯èƒ½çš„ç©ºæ ¼å’Œå†’å·ï¼‰
        payload_hex = payload_hex.replace(" ", "").replace(":", "")
        
        # Byte Balancing for payload
        if payload_hex:
            payload_hex = payload_hex[:self.config['payload_bytes'] * 2]
        
        return {
            'flow_key': flow_key,
            'ts': ts,
            'len': length,
            'payload': payload_hex,
            'direction': direction,
            'original_src': original_src,
            'original_dst': original_dst,
            'src_port': sport,
            'dst_port': dport,
            'protocol': protocol
        }
    
    def _post_process_flows(self, flows):
        """æµåå¤„ç†"""
        processed = {}
        
        for key, packets in flows.items():
            # æŒ‰æ—¶é—´æ’åº
            packets = sorted(packets, key=lambda x: x['ts'])
            
            # é™åˆ¶åŒ…æ•°é‡
            max_pkts = self.config['max_packets_per_flow']
            if len(packets) > max_pkts:
                packets = packets[:max_pkts]
            
            # ç®€åŒ–ä¸ºæœ€ç»ˆæ ¼å¼
            processed[key] = [{
                'ts': p['ts'],
                'len': p['len'],
                'payload': p['payload'],
                'direction': p['direction']
            } for p in packets]
        
        return processed


# ==================== PCAPæ¨¡å¼å¤„ç† ====================

class PcapFlowBuilder:
    """
    ç›´æ¥è§£æPCAPçš„æµæ„å»ºå™¨ï¼ˆNetMambaé£æ ¼ï¼‰
    
    ä¼˜åŠ¿:
    - å¯ä»¥æå–å®Œæ•´çš„Header
    - ç²¾ç¡®çš„Payloadåˆ†ç¦»
    - æ”¯æŒIP Masking
    """
    
    def __init__(self, config=None):
        self.config = {**DEFAULT_CONFIG, **(config or {})}
        self._check_scapy()
    
    def _check_scapy(self):
        """æ£€æŸ¥scapyæ˜¯å¦å¯ç”¨"""
        try:
            import scapy.all as scapy
            self.scapy = scapy
            return True
        except ImportError:
            print("[!] Warning: scapy not installed. PCAP mode not available.")
            print("    Install with: pip install scapy")
            self.scapy = None
            return False
    
    def build_flows(self, pcap_file):
        """
        ä»PCAPæ„å»ºå•å‘æµï¼ˆNetMambaé£æ ¼ï¼‰

        Args:
            pcap_file: PCAPæ–‡ä»¶è·¯å¾„

        Returns:
            flows: dict, æµå­—å…¸
            stats: dict, ç»Ÿè®¡ä¿¡æ¯
        """
        if self.scapy is None:
            raise ImportError("scapy is required for PCAP mode")

        flows = defaultdict(list)
        flow_counters = defaultdict(int)  # ğŸ†• è®°å½•æ¯ä¸ªåŸºç¡€æµçš„åºå·
        stats = {
            'total_packets': 0,
            'valid_packets': 0,
            'skipped_no_ip': 0,
            'skipped_empty_payload': 0,
            'flows_split_by_timeout': 0  # ğŸ†• å› è¶…æ—¶åˆ†å‰²çš„æµæ•°é‡
        }
        
        print(f"[+] Reading PCAP: {pcap_file}")
        packets = self.scapy.rdpcap(str(pcap_file))
        stats['total_packets'] = len(packets)
        
        for pkt in packets:
            result = self._process_packet(pkt)
            
            if result is None:
                stats['skipped_no_ip'] += 1
                continue
            
            if self.config['filter_empty_payload'] and not result['payload']:
                stats['skipped_empty_payload'] += 1
                continue

            # ğŸ†• æ£€æŸ¥æµè¶…æ—¶
            if self.config['enable_timeout']:
                base_key = result['flow_key'][:5]  # ä¸å«directionçš„äº”å…ƒç»„

                # æ£€æŸ¥æ˜¯å¦éœ€è¦å› è¶…æ—¶åˆ†å‰²æµ
                if result['flow_key'] in flows and flows[result['flow_key']]:
                    if should_split_flow(flows[result['flow_key']], result['ts'],
                                        self.config['flow_timeout']):
                        # åˆ›å»ºæ–°çš„æµkey (æ·»åŠ æµåºå·)
                        flow_counters[base_key] += 1
                        # æ–°keyæ ¼å¼: (ip1, ip2, port1, port2, proto, direction, seq)
                        new_key = (*result['flow_key'], flow_counters[base_key])
                        result['flow_key'] = new_key
                        stats['flows_split_by_timeout'] += 1

            stats['valid_packets'] += 1
            flows[result['flow_key']].append(result)
        
        # åå¤„ç†
        flows = self._post_process_flows(dict(flows))
        
        return flows, stats
    
    def _process_packet(self, pkt):
        """å¤„ç†å•ä¸ªåŒ…ï¼ˆNetMambaé£æ ¼ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æœ‰IPå±‚
        if not pkt.haslayer('IP'):
            return None
        
        ip_layer = pkt['IP']
        
        # æå–IPä¿¡æ¯
        src = ip_layer.src
        dst = ip_layer.dst
        protocol = ip_layer.proto
        
        # æå–ç«¯å£
        sport, dport = 0, 0
        if pkt.haslayer('TCP'):
            sport = pkt['TCP'].sport
            dport = pkt['TCP'].dport
        elif pkt.haslayer('UDP'):
            sport = pkt['UDP'].sport
            dport = pkt['UDP'].dport
        
        # æ–¹å‘å’Œæµkeyï¼ˆä½¿ç”¨åŸå§‹IPï¼‰
        direction = determine_direction(src, dst)
        if direction == 0:
            flow_key = (src, dst, sport, dport, protocol, 0)
        else:
            flow_key = (dst, src, dport, sport, protocol, 1)
        
        # æ—¶é—´æˆ³
        ts = float(pkt.time)
        
        # åŒ…é•¿åº¦
        length = len(pkt)
        
        # æå–Headerå’ŒPayloadï¼ˆNetMambaæ–¹å¼ï¼‰
        header_hex, payload_hex = self._extract_header_payload(pkt)
        
        return {
            'flow_key': flow_key,
            'ts': ts,
            'len': length,
            'header': header_hex,
            'payload': payload_hex,
            'direction': direction,
            'original_src': src,
            'original_dst': dst,
            'src_port': sport,
            'dst_port': dport,
            'protocol': protocol
        }
    
    def _extract_header_payload(self, pkt):
        """
        æå–Headerå’ŒPayloadï¼ˆå‚è€ƒNetMambaçš„å®ç°ï¼‰
        
        Header: IPå±‚å­—èŠ‚ï¼ˆå»é™¤payloadéƒ¨åˆ†ï¼‰
        Payload: Rawå±‚å­—èŠ‚
        """
        ip_layer = pkt['IP']
        
        # å¯¹IPè¿›è¡ŒMaskingï¼ˆåˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰
        if self.config['ip_masking']:
            ip_layer = ip_layer.copy()
            ip_layer.src = self.config['mask_ip']
            ip_layer.dst = self.config['mask_ip']
        
        # æå–headerï¼ˆæ•´ä¸ªIPåŒ…çš„å­—èŠ‚ï¼‰
        try:
            header_bytes = bytes(ip_layer)
            header_hex = binascii.hexlify(header_bytes).decode()
        except:
            header_hex = ''
        
        # æå–payload
        try:
            if pkt.haslayer('Raw'):
                payload_bytes = bytes(pkt['Raw'])
                payload_hex = binascii.hexlify(payload_bytes).decode()
                # ä»headerä¸­ç§»é™¤payload
                header_hex = header_hex.replace(payload_hex, '')
            else:
                payload_hex = ''
        except:
            payload_hex = ''
        
        # Byte Balancing
        header_len = self.config['header_bytes'] * 2  # åå…­è¿›åˆ¶å­—ç¬¦æ•°
        payload_len = self.config['payload_bytes'] * 2
        
        # Header: æˆªæ–­æˆ–å¡«å……
        if len(header_hex) > header_len:
            header_hex = header_hex[:header_len]
        else:
            header_hex = header_hex + '0' * (header_len - len(header_hex))
        
        # Payload: æˆªæ–­æˆ–å¡«å……
        if len(payload_hex) > payload_len:
            payload_hex = payload_hex[:payload_len]
        else:
            payload_hex = payload_hex + '0' * (payload_len - len(payload_hex))
        
        return header_hex, payload_hex
    
    def _post_process_flows(self, flows):
        """æµåå¤„ç†"""
        processed = {}
        
        for key, packets in flows.items():
            # æŒ‰æ—¶é—´æ’åº
            packets = sorted(packets, key=lambda x: x['ts'])
            
            # é™åˆ¶åŒ…æ•°é‡
            max_pkts = self.config['max_packets_per_flow']
            if len(packets) > max_pkts:
                packets = packets[:max_pkts]
            
            # ä¿å­˜ä¸ºæœ€ç»ˆæ ¼å¼
            processed[key] = [{
                'ts': p['ts'],
                'len': p['len'],
                'header': p.get('header', ''),
                'payload': p.get('payload', ''),
                'direction': p['direction']
            } for p in packets]
        
        return processed


# ==================== NetMambaæ ¼å¼è½¬æ¢ ====================

def convert_to_netmamba_format(flows, config=None):
    """
    å°†æµè½¬æ¢ä¸ºNetMambaæ ¼å¼çš„numpyæ•°ç»„
    
    NetMambaæ ¼å¼:
    - æ¯ä¸ªæµ: 5ä¸ªåŒ… Ã— 320å­—èŠ‚/åŒ… = 1600å­—èŠ‚
    - æ¯ä¸ªåŒ…: 80å­—èŠ‚header + 240å­—èŠ‚payload
    
    Args:
        flows: æµå­—å…¸
        config: é…ç½®å‚æ•°
    
    Returns:
        dict: {flow_key: numpy.array([1600,], dtype=uint8)}
    """
    config = {**DEFAULT_CONFIG, **(config or {})}
    
    netmamba_flows = {}
    
    num_packets = config['netmamba_packets']  # 5
    header_bytes = config['header_bytes']      # 80
    payload_bytes = config['payload_bytes']    # 240
    packet_bytes = header_bytes + payload_bytes  # 320
    flow_bytes = num_packets * packet_bytes      # 1600
    
    for key, packets in flows.items():
        # å–å‰5ä¸ªåŒ…
        packets = packets[:num_packets]
        
        # æ„å»ºå­—èŠ‚åºåˆ—
        flow_hex_list = []
        
        for i in range(num_packets):
            if i < len(packets):
                pkt = packets[i]
                # æå–headerå’Œpayload
                header = pkt.get('header', '0' * header_bytes * 2)
                payload = pkt.get('payload', '0' * payload_bytes * 2)
                
                # Balancing
                header = balance_bytes(header, header_bytes)
                payload = balance_bytes(payload, payload_bytes)
                
                # è¿æ¥
                pkt_bytes = np.concatenate([header, payload])
            else:
                # ä¸è¶³5ä¸ªåŒ…æ—¶ç”¨0å¡«å……
                pkt_bytes = np.zeros(packet_bytes, dtype=np.uint8)
            
            flow_hex_list.append(pkt_bytes)
        
        # è¿æ¥æˆä¸€ä¸ªæ•°ç»„
        flow_array = np.concatenate(flow_hex_list)
        assert len(flow_array) == flow_bytes, f"Expected {flow_bytes}, got {len(flow_array)}"
        
        netmamba_flows[key] = flow_array
    
    return netmamba_flows


# ==================== ä¸»ç¨‹åº ====================

def parse_args():
    parser = argparse.ArgumentParser(
        description="å•å‘æµæ„å»ºå™¨ V2 - æ•´åˆNFStreamå’ŒNetMambaæ€è·¯",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»tshark CSVæ„å»ºæµï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
  python build_unidirectional_flows_v2.py --csv packets.csv --out flows.pkl
  
  # ä»PCAPæ„å»ºæµï¼ˆNetMambaæ¨¡å¼ï¼‰
  python build_unidirectional_flows_v2.py --pcap Monday.pcap --out flows.pkl --mode netmamba
  
  # å¯ç”¨IP Masking
  python build_unidirectional_flows_v2.py --csv packets.csv --out flows.pkl --ip-masking
  
  # é™åˆ¶æ¯æµ5ä¸ªåŒ…ï¼ˆNetMambaé£æ ¼ï¼‰
  python build_unidirectional_flows_v2.py --csv packets.csv --out flows.pkl --max-packets 5
  
  # è¾“å‡ºNetMambaæ ¼å¼ï¼ˆ1600å­—èŠ‚æ•°ç»„ï¼‰
  python build_unidirectional_flows_v2.py --pcap Monday.pcap --out flows.pkl --format netmamba
        """
    )
    
    # è¾“å…¥æºï¼ˆäºŒé€‰ä¸€ï¼‰
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--csv", help="Input packet-level CSV from tshark")
    input_group.add_argument("--pcap", help="Input PCAP file (requires scapy)")
    
    # è¾“å‡º
    parser.add_argument("--out", required=True, help="Output .pkl file")
    
    # å¤„ç†é€‰é¡¹
    parser.add_argument("--max-packets", type=int, default=100,
                        help="Maximum packets per flow (default: 100)")
    parser.add_argument("--header-bytes", type=int, default=80,
                        help="Header length in bytes (default: 80)")
    parser.add_argument("--payload-bytes", type=int, default=240,
                        help="Payload length in bytes (default: 240)")
    
    # IP Masking
    parser.add_argument("--ip-masking", action="store_true",
                        help="Enable IP masking (set IP to 0.0.0.0)")
    parser.add_argument("--no-ip-masking", action="store_true",
                        help="Disable IP masking")
    
    # è¿‡æ»¤é€‰é¡¹
    parser.add_argument("--filter-empty", action="store_true",
                        help="Filter packets without payload")

    # ğŸ†• æµè¶…æ—¶å‚æ•°
    parser.add_argument("--flow-timeout", type=int, default=120,
                        help="Flow timeout in seconds. Split flow if idle > timeout (default: 120)")
    parser.add_argument("--no-timeout", action="store_true",
                        help="Disable flow timeout mechanism")

    # è¾“å‡ºæ ¼å¼
    parser.add_argument("--format", choices=['dict', 'netmamba', 'both'],
                        default='dict',
                        help="Output format: dict (default), netmamba (1600-byte arrays), both")
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # æ„å»ºé…ç½®
    config = {
        'max_packets_per_flow': args.max_packets,
        'header_bytes': args.header_bytes,
        'payload_bytes': args.payload_bytes,
        'ip_masking': args.ip_masking and not args.no_ip_masking,
        'filter_empty_payload': args.filter_empty,
        # ğŸ†• è¶…æ—¶é…ç½®
        'flow_timeout': args.flow_timeout,
        'enable_timeout': not args.no_timeout,
    }
    
    print("=" * 60)
    print("å•å‘æµæ„å»ºå™¨ V2")
    print("=" * 60)
    print(f"é…ç½®:")
    print(f"  - æœ€å¤§åŒ…æ•°/æµ: {config['max_packets_per_flow']}")
    print(f"  - Headeré•¿åº¦: {config['header_bytes']} bytes")
    print(f"  - Payloadé•¿åº¦: {config['payload_bytes']} bytes")
    print(f"  - IP Masking: {config['ip_masking']}")
    print(f"  - è¿‡æ»¤ç©ºpayload: {config['filter_empty_payload']}")
    print(f"  - æµè¶…æ—¶: {config['flow_timeout']}ç§’ (å¯ç”¨: {config['enable_timeout']})")
    print(f"  - è¾“å‡ºæ ¼å¼: {args.format}")
    print("=" * 60)
    
    # é€‰æ‹©å¤„ç†æ¨¡å¼
    if args.csv:
        print(f"\n[+] æ¨¡å¼: CSV (tshark)")
        print(f"[+] è¾“å…¥: {args.csv}")
        builder = CSVFlowBuilder(config)
        flows, stats = builder.build_flows(args.csv)
    else:
        print(f"\n[+] æ¨¡å¼: PCAP (ç›´æ¥è§£æ)")
        print(f"[+] è¾“å…¥: {args.pcap}")
        builder = PcapFlowBuilder(config)
        flows, stats = builder.build_flows(args.pcap)
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\n[+] ç»Ÿè®¡:")
    print(f"  - æ€»åŒ…æ•°: {stats['total_packets']:,}")
    print(f"  - æœ‰æ•ˆåŒ…: {stats['valid_packets']:,}")
    if 'skipped_invalid' in stats:
        print(f"  - è·³è¿‡(æ— æ•ˆ): {stats['skipped_invalid']:,}")
    if 'skipped_no_ip' in stats:
        print(f"  - è·³è¿‡(æ— IP): {stats['skipped_no_ip']:,}")
    if 'skipped_empty_payload' in stats:
        print(f"  - è·³è¿‡(æ— payload): {stats['skipped_empty_payload']:,}")
    if 'flows_split_by_timeout' in stats and stats['flows_split_by_timeout'] > 0:
        print(f"  - å› è¶…æ—¶åˆ†å‰²çš„æµ: {stats['flows_split_by_timeout']:,}")
    print(f"  - æ€»æµæ•°: {len(flows):,}")
    
    # ä¿å­˜ç»“æœ
    output_data = {'flows': flows, 'config': config, 'stats': stats}
    
    # å¦‚æœéœ€è¦NetMambaæ ¼å¼
    if args.format in ['netmamba', 'both']:
        print(f"\n[+] è½¬æ¢ä¸ºNetMambaæ ¼å¼...")
        netmamba_flows = convert_to_netmamba_format(flows, config)
        output_data['netmamba_flows'] = netmamba_flows
        print(f"  - NetMambaæµæ•°: {len(netmamba_flows):,}")
        print(f"  - æ¯æµå­—èŠ‚æ•°: 1600")
    
    print(f"\n[+] ä¿å­˜åˆ°: {args.out}")
    with open(args.out, "wb") as f:
        pickle.dump(output_data, f)
    
    # éªŒè¯è¾“å‡º
    print(f"\n[+] éªŒè¯:")
    if flows:
        sample_key = list(flows.keys())[0]
        sample_flow = flows[sample_key]
        print(f"  - ç¤ºä¾‹æµkey: {sample_key}")
        print(f"  - åŒ…æ•°: {len(sample_flow)}")
        if sample_flow:
            print(f"  - ç¬¬ä¸€ä¸ªåŒ…:")
            for k, v in sample_flow[0].items():
                if k == 'payload' and v:
                    print(f"      {k}: {v[:40]}... ({len(v)//2} bytes)")
                elif k == 'header' and v:
                    print(f"      {k}: {v[:40]}... ({len(v)//2} bytes)")
                else:
                    print(f"      {k}: {v}")
    
    print("\n[+] å®Œæˆ!")


if __name__ == "__main__":
    main()
