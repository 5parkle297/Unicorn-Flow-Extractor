#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CICIDS2017 é¢„å¤„ç†è´¨é‡è¯„ä¼°è„šæœ¬
Preprocessing Quality Evaluation Script

è¯„ä¼°8ä¸ªç»´åº¦:
1. æ•°æ®å®Œæ•´æ€§ (Data Integrity)
2. äº”å…ƒç»„æ­£ç¡®æ€§ (5-tuple Accuracy)
3. Payloadå®Œæ•´æ€§ (Payload Integrity)
4. ç»Ÿè®¡ç‰¹å¾å‡†ç¡®æ€§ (Feature Accuracy)
5. ä¸å®˜æ–¹æ•°æ®é›†å¯¹æ¯” (Baseline Comparison)
6. æ ‡ç­¾åŒ¹é…éªŒè¯ (Label Matching)
7. æ•°æ®åˆ†å¸ƒåˆ†æ (Distribution Analysis)
8. æœºå™¨å­¦ä¹ åŸºå‡†æµ‹è¯• (ML Benchmark)
"""

import pickle
import pandas as pd
import numpy as np
import random
import subprocess
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
    from sklearn.metrics import confusion_matrix, classification_report
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸  sklearnæœªå®‰è£…ï¼Œæœºå™¨å­¦ä¹ æµ‹è¯•å°†è·³è¿‡")


class PreprocessingEvaluator:
    """é¢„å¤„ç†è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, flows_pkl, csv_file=None, pcap_file=None, official_csv=None):
        """
        Args:
            flows_pkl: å•å‘æµpklæ–‡ä»¶
            csv_file: æå–çš„ç‰¹å¾CSVæ–‡ä»¶
            pcap_file: åŸå§‹PCAPæ–‡ä»¶
            official_csv: å®˜æ–¹CICIDS2017 CSVæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        """
        self.flows_pkl = flows_pkl
        self.csv_file = csv_file
        self.pcap_file = pcap_file
        self.official_csv = official_csv
        
        # åŠ è½½æ•°æ®
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {flows_pkl}")
        with open(flows_pkl, 'rb') as f:
            self.flows = pickle.load(f)
        print(f"   âœ“ åŠ è½½äº† {len(self.flows)} ä¸ªæµ")
        
        if csv_file and Path(csv_file).exists():
            self.df = pd.read_csv(csv_file)
            print(f"   âœ“ åŠ è½½äº† {len(self.df)} æ¡CSVè®°å½•")
        else:
            self.df = None
            
        self.results = {}
        
    def evaluate_all(self):
        """æ‰§è¡Œæ‰€æœ‰è¯„ä¼°"""
        print("\n" + "="*80)
        print("ğŸ¯ å¼€å§‹é¢„å¤„ç†è´¨é‡è¯„ä¼°")
        print("="*80)
        
        # 1. æ•°æ®å®Œæ•´æ€§
        print("\n[1/8] è¯„ä¼°æ•°æ®å®Œæ•´æ€§...")
        self.evaluate_completeness()
        
        # 2. äº”å…ƒç»„æ­£ç¡®æ€§
        print("\n[2/8] è¯„ä¼°äº”å…ƒç»„æ­£ç¡®æ€§...")
        self.evaluate_5tuple()
        
        # 3. Payloadå®Œæ•´æ€§
        print("\n[3/8] è¯„ä¼°Payloadå®Œæ•´æ€§...")
        self.evaluate_payload()
        
        # 4. ç»Ÿè®¡ç‰¹å¾å‡†ç¡®æ€§
        print("\n[4/8] è¯„ä¼°ç»Ÿè®¡ç‰¹å¾å‡†ç¡®æ€§...")
        self.evaluate_features()
        
        # 5. ä¸å®˜æ–¹æ•°æ®é›†å¯¹æ¯”
        if self.official_csv and Path(self.official_csv).exists():
            print("\n[5/8] ä¸å®˜æ–¹æ•°æ®é›†å¯¹æ¯”...")
            self.compare_with_official()
        else:
            print("\n[5/8] è·³è¿‡å®˜æ–¹æ•°æ®é›†å¯¹æ¯”ï¼ˆæœªæä¾›å®˜æ–¹CSVï¼‰")
            self.results['official_comparison'] = {'skipped': True}
        
        # 6. æ ‡ç­¾åŒ¹é…éªŒè¯
        print("\n[6/8] è¯„ä¼°æ ‡ç­¾åŒ¹é…...")
        self.evaluate_labels()
        
        # 7. æ•°æ®åˆ†å¸ƒåˆ†æ
        print("\n[7/8] åˆ†ææ•°æ®åˆ†å¸ƒ...")
        self.analyze_distribution()
        
        # 8. æœºå™¨å­¦ä¹ åŸºå‡†æµ‹è¯•
        if ML_AVAILABLE and self.df is not None:
            print("\n[8/8] æœºå™¨å­¦ä¹ åŸºå‡†æµ‹è¯•...")
            self.ml_benchmark()
        else:
            print("\n[8/8] è·³è¿‡æœºå™¨å­¦ä¹ æµ‹è¯•")
            self.results['ml_benchmark'] = {'skipped': True}
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\n" + "="*80)
        print("ğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        self.generate_report()
        
        return self.results
    
    def evaluate_completeness(self):
        """1. æ•°æ®å®Œæ•´æ€§éªŒè¯"""
        results = {}
        
        # ç»Ÿè®¡æµä¸­çš„åŒ…æ•°
        total_packets = sum(len(pkts) for pkts in self.flows.values())
        results['flow_count'] = len(self.flows)
        results['total_packets'] = total_packets
        
        # æµå¤§å°åˆ†å¸ƒ
        flow_sizes = [len(pkts) for pkts in self.flows.values()]
        results['flow_size_stats'] = {
            'min': int(np.min(flow_sizes)),
            'max': int(np.max(flow_sizes)),
            'mean': float(np.mean(flow_sizes)),
            'median': float(np.median(flow_sizes)),
            'std': float(np.std(flow_sizes))
        }
        
        # åè®®åˆ†å¸ƒ
        tcp_count = sum(1 for key in self.flows.keys() if key[4] == 6)
        udp_count = sum(1 for key in self.flows.keys() if key[4] == 17)
        other_count = len(self.flows) - tcp_count - udp_count
        
        results['protocol_distribution'] = {
            'TCP': tcp_count,
            'UDP': udp_count,
            'Other': other_count
        }
        
        # å•å‘æµæ–¹å‘åˆ†å¸ƒ
        direction_0 = sum(1 for key in self.flows.keys() if key[5] == 0)
        direction_1 = sum(1 for key in self.flows.keys() if key[5] == 1)
        results['direction_distribution'] = {
            'direction_0': direction_0,
            'direction_1': direction_1
        }
        
        # å¦‚æœæœ‰PCAPæ–‡ä»¶ï¼Œå¯¹æ¯”åŒ…æ•°
        if self.pcap_file and Path(self.pcap_file).exists():
            pcap_packets = self.count_pcap_packets()
            if pcap_packets > 0:
                coverage = (total_packets / pcap_packets) * 100
                results['pcap_packets'] = pcap_packets
                results['coverage_rate'] = round(coverage, 2)
                results['coverage_pass'] = coverage > 90
        
        # æ—¶é—´èŒƒå›´
        all_timestamps = []
        for pkts in self.flows.values():
            all_timestamps.extend([p['ts'] for p in pkts])
        
        if all_timestamps:
            results['time_range'] = {
                'start': float(min(all_timestamps)),
                'end': float(max(all_timestamps)),
                'duration_seconds': float(max(all_timestamps) - min(all_timestamps))
            }
        
        self.results['completeness'] = results
        
        print(f"   âœ“ æµæ•°é‡: {results['flow_count']}")
        print(f"   âœ“ æ€»åŒ…æ•°: {results['total_packets']}")
        print(f"   âœ“ TCPæµ: {tcp_count}, UDPæµ: {udp_count}")
        if 'coverage_rate' in results:
            print(f"   âœ“ PCAPè¦†ç›–ç‡: {results['coverage_rate']}% {'âœ“ PASS' if results['coverage_pass'] else 'âœ— FAIL'}")
    
    def evaluate_5tuple(self):
        """2. äº”å…ƒç»„æ­£ç¡®æ€§éªŒè¯ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰"""
        results = {}
        
        # éšæœºæŠ½æ ·100ä¸ªæµï¼ˆæˆ–å…¨éƒ¨ï¼Œå¦‚æœå°‘äº100ï¼‰
        sample_size = min(100, len(self.flows))
        sampled_keys = random.sample(list(self.flows.keys()), sample_size)
        
        valid_count = 0
        errors = []
        
        for key in sampled_keys:
            src_ip, dst_ip, src_port, dst_port, protocol, direction = key
            packets = self.flows[key]
            
            # æ£€æŸ¥äº”å…ƒç»„ä¸€è‡´æ€§
            is_valid = True
            
            # æ£€æŸ¥ç«¯å£åˆæ³•æ€§
            if not (0 <= src_port <= 65535 and 0 <= dst_port <= 65535):
                is_valid = False
                errors.append({
                    'flow': key,
                    'error': 'Invalid port number'
                })
            
            # æ£€æŸ¥åè®®åˆæ³•æ€§
            if protocol not in [6, 17]:  # TCP, UDP
                is_valid = False
                errors.append({
                    'flow': key,
                    'error': f'Invalid protocol: {protocol}'
                })
            
            # æ£€æŸ¥æ–¹å‘åˆæ³•æ€§
            if direction not in [0, 1]:
                is_valid = False
                errors.append({
                    'flow': key,
                    'error': f'Invalid direction: {direction}'
                })
            
            # æ£€æŸ¥åŒ…æ•°é‡
            if len(packets) == 0:
                is_valid = False
                errors.append({
                    'flow': key,
                    'error': 'Empty flow'
                })
            
            if is_valid:
                valid_count += 1
        
        accuracy = (valid_count / sample_size) * 100
        results['sample_size'] = sample_size
        results['valid_count'] = valid_count
        results['accuracy'] = round(accuracy, 2)
        results['pass'] = accuracy > 99
        results['errors'] = errors[:10]  # åªä¿ç•™å‰10ä¸ªé”™è¯¯
        
        self.results['5tuple_accuracy'] = results
        
        print(f"   âœ“ æŠ½æ ·æ•°é‡: {sample_size}")
        print(f"   âœ“ æœ‰æ•ˆæµ: {valid_count}")
        print(f"   âœ“ å‡†ç¡®ç‡: {accuracy}% {'âœ“ PASS' if results['pass'] else 'âœ— FAIL'}")
    
    def evaluate_payload(self):
        """3. Payloadå®Œæ•´æ€§éªŒè¯ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰"""
        results = {}
        
        # éšæœºæŠ½æ ·50ä¸ªåŒ…
        sample_size = min(50, sum(len(pkts) for pkts in self.flows.values()))
        
        # æ”¶é›†æ‰€æœ‰åŒ…
        all_packets = []
        for flow_key, pkts in self.flows.items():
            for pkt in pkts:
                all_packets.append((flow_key, pkt))
        
        sampled_packets = random.sample(all_packets, min(sample_size, len(all_packets)))
        
        valid_count = 0
        empty_payload_count = 0
        
        for flow_key, pkt in sampled_packets:
            payload = pkt.get('payload', '')
            
            # æ£€æŸ¥payloadæ˜¯å¦ä¸ºæœ‰æ•ˆçš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²
            is_valid = True
            
            if not payload:
                empty_payload_count += 1
                is_valid = True  # ç©ºpayloadæ˜¯åˆæ³•çš„
            elif not all(c in '0123456789abcdefABCDEF' for c in payload):
                is_valid = False
            
            if is_valid:
                valid_count += 1
        
        match_rate = (valid_count / len(sampled_packets)) * 100 if sampled_packets else 100
        
        results['sample_size'] = len(sampled_packets)
        results['valid_count'] = valid_count
        results['empty_payload_count'] = empty_payload_count
        results['match_rate'] = round(match_rate, 2)
        results['pass'] = match_rate == 100
        
        self.results['payload_integrity'] = results
        
        print(f"   âœ“ æŠ½æ ·æ•°é‡: {len(sampled_packets)}")
        print(f"   âœ“ æœ‰æ•ˆpayload: {valid_count}")
        print(f"   âœ“ ç©ºpayload: {empty_payload_count}")
        print(f"   âœ“ åŒ¹é…ç‡: {match_rate}% {'âœ“ PASS' if results['pass'] else 'âœ— FAIL'}")
    
    def evaluate_features(self):
        """4. ç»Ÿè®¡ç‰¹å¾å‡†ç¡®æ€§éªŒè¯"""
        if self.df is None:
            print("   âš ï¸  æœªæä¾›CSVæ–‡ä»¶ï¼Œè·³è¿‡ç‰¹å¾éªŒè¯")
            self.results['feature_accuracy'] = {'skipped': True}
            return
        
        results = {}
        errors = []
        
        # éšæœºæŠ½æ ·æ£€æŸ¥
        sample_size = min(100, len(self.df))
        sampled_indices = random.sample(range(len(self.df)), sample_size)
        
        error_count = 0
        
        for idx in sampled_indices:
            row = self.df.iloc[idx]
            
            # å°è¯•ä»rowä¸­è·å–äº”å…ƒç»„ä¿¡æ¯é‡å»ºflow_key
            # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„CSVæ ¼å¼è°ƒæ•´
            # å‡è®¾CSVä¸­æœ‰ src_ip, dst_ip, src_port, dst_port, protocolç­‰å­—æ®µ
            
            # ç”±äºCSVæ ¼å¼å¯èƒ½ä¸åŒï¼Œè¿™é‡Œåšç®€å•çš„ä¸€è‡´æ€§æ£€æŸ¥
            # æ£€æŸ¥æ•°å€¼ç‰¹å¾æ˜¯å¦åˆç†
            
            checks = {
                'total_packets': row.get('total_packets', 0) > 0,
                'total_bytes': row.get('total_bytes', 0) >= 0,
                'duration': row.get('duration', 0) >= 0,
            }
            
            if not all(checks.values()):
                error_count += 1
                errors.append({
                    'index': idx,
                    'failed_checks': [k for k, v in checks.items() if not v]
                })
        
        accuracy = ((sample_size - error_count) / sample_size) * 100 if sample_size > 0 else 100
        error_rate = (error_count / sample_size) * 100 if sample_size > 0 else 0
        
        results['sample_size'] = sample_size
        results['error_count'] = error_count
        results['accuracy'] = round(accuracy, 2)
        results['error_rate'] = round(error_rate, 2)
        results['pass'] = error_rate < 1
        results['errors'] = errors[:10]
        
        self.results['feature_accuracy'] = results
        
        print(f"   âœ“ æŠ½æ ·æ•°é‡: {sample_size}")
        print(f"   âœ“ é”™è¯¯æ•°é‡: {error_count}")
        print(f"   âœ“ è¯¯å·®ç‡: {error_rate}% {'âœ“ PASS' if results['pass'] else 'âœ— FAIL'}")
    
    def compare_with_official(self):
        """5. ä¸å®˜æ–¹æ•°æ®é›†å¯¹æ¯”"""
        try:
            official_df = pd.read_csv(self.official_csv)
            
            results = {
                'our_flow_count': len(self.df) if self.df is not None else len(self.flows),
                'official_flow_count': len(official_df),
                'note': 'å•å‘æµæ•°é‡çº¦ä¸ºåŒå‘æµçš„2å€'
            }
            
            self.results['official_comparison'] = results
            
            print(f"   âœ“ æˆ‘ä»¬çš„æµæ•°é‡: {results['our_flow_count']}")
            print(f"   âœ“ å®˜æ–¹æµæ•°é‡: {results['official_flow_count']}")
            print(f"   â„¹ï¸  {results['note']}")
            
        except Exception as e:
            print(f"   âš ï¸  å¯¹æ¯”å¤±è´¥: {str(e)}")
            self.results['official_comparison'] = {'error': str(e)}
    
    def evaluate_labels(self):
        """6. æ ‡ç­¾åŒ¹é…éªŒè¯"""
        if self.df is None or 'label' not in self.df.columns:
            print("   âš ï¸  CSVä¸­æ— labelå­—æ®µï¼Œè·³è¿‡æ ‡ç­¾éªŒè¯")
            self.results['label_matching'] = {'skipped': True}
            return
        
        results = {}
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        label_dist = self.df['label'].value_counts().to_dict()
        results['label_distribution'] = label_dist
        results['unique_labels'] = len(label_dist)
        results['total_samples'] = len(self.df)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±æ ‡ç­¾
        missing_labels = self.df['label'].isna().sum()
        results['missing_labels'] = int(missing_labels)
        results['missing_rate'] = round((missing_labels / len(self.df)) * 100, 2)
        
        results['pass'] = missing_labels == 0
        
        self.results['label_matching'] = results
        
        print(f"   âœ“ æ ‡ç­¾ç§ç±»: {results['unique_labels']}")
        print(f"   âœ“ ç¼ºå¤±æ ‡ç­¾: {missing_labels} ({results['missing_rate']}%)")
        print(f"   âœ“ {'âœ“ PASS' if results['pass'] else 'âœ— FAIL'}")
    
    def analyze_distribution(self):
        """7. æ•°æ®åˆ†å¸ƒåˆ†æ"""
        if self.df is None:
            print("   âš ï¸  æœªæä¾›CSVæ–‡ä»¶ï¼Œè·³è¿‡åˆ†å¸ƒåˆ†æ")
            self.results['distribution_analysis'] = {'skipped': True}
            return
        
        results = {}
        
        # æ•°å€¼åˆ—ç»Ÿè®¡
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        
        stats = {}
        for col in numeric_cols[:10]:  # åªåˆ†æå‰10ä¸ªæ•°å€¼åˆ—
            stats[col] = {
                'mean': float(self.df[col].mean()),
                'std': float(self.df[col].std()),
                'min': float(self.df[col].min()),
                'max': float(self.df[col].max()),
                'zeros': int((self.df[col] == 0).sum()),
                'nulls': int(self.df[col].isna().sum())
            }
        
        results['numeric_stats'] = stats
        
        # æ€»ä½“ç¼ºå¤±å€¼
        results['total_nulls'] = int(self.df.isna().sum().sum())
        results['null_percentage'] = round((results['total_nulls'] / (len(self.df) * len(self.df.columns))) * 100, 2)
        
        # å¼‚å¸¸æ£€æµ‹ï¼ˆç®€å•çš„3-sigmaè§„åˆ™ï¼‰
        outlier_counts = {}
        for col in numeric_cols[:10]:
            mean = self.df[col].mean()
            std = self.df[col].std()
            if std > 0:
                outliers = ((self.df[col] - mean).abs() > 3 * std).sum()
                outlier_counts[col] = int(outliers)
        
        results['outlier_counts'] = outlier_counts
        results['pass'] = results['null_percentage'] < 5  # ç¼ºå¤±å€¼æ¯”ä¾‹å°äº5%
        
        self.results['distribution_analysis'] = results
        
        print(f"   âœ“ åˆ†æäº† {len(stats)} ä¸ªæ•°å€¼ç‰¹å¾")
        print(f"   âœ“ ç¼ºå¤±å€¼: {results['total_nulls']} ({results['null_percentage']}%)")
        print(f"   âœ“ {'âœ“ PASS' if results['pass'] else 'âœ— FAIL'}")
    
    def ml_benchmark(self):
        """8. æœºå™¨å­¦ä¹ åŸºå‡†æµ‹è¯•"""
        if self.df is None or 'label' not in self.df.columns:
            print("   âš ï¸  æ— æ³•è¿›è¡ŒMLæµ‹è¯•")
            self.results['ml_benchmark'] = {'skipped': True}
            return
        
        try:
            # å‡†å¤‡æ•°æ®
            df_clean = self.df.copy()
            
            # ç§»é™¤éæ•°å€¼åˆ—å’Œæ ‡ç­¾åˆ—
            X = df_clean.select_dtypes(include=[np.number])
            y = df_clean['label']
            
            # ç§»é™¤labelå¦‚æœå®ƒåœ¨Xä¸­
            if 'label' in X.columns:
                X = X.drop('label', axis=1)
            
            # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(0)
            
            # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥åŠ å¿«é€Ÿåº¦
            max_samples = min(10000, len(X))
            if len(X) > max_samples:
                indices = random.sample(range(len(X)), max_samples)
                X = X.iloc[indices]
                y = y.iloc[indices]
            
            # åˆ†å‰²æ•°æ®
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y if len(y.unique()) > 1 else None
            )
            
            print(f"   è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")
            
            # è®­ç»ƒRandomForest
            print("   è®­ç»ƒRandomForest...")
            rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10)
            rf.fit(X_train, y_train)
            
            y_pred = rf.predict(X_test)
            
            # è®¡ç®—æŒ‡æ ‡
            results = {
                'classifier': 'RandomForest',
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'features_used': len(X.columns),
                'accuracy': round(accuracy_score(y_test, y_pred) * 100, 2),
                'precision': round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 2),
                'recall': round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 2),
                'f1_score': round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 2),
            }
            
            results['pass'] = results['accuracy'] > 85
            
            self.results['ml_benchmark'] = results
            
            print(f"   âœ“ å‡†ç¡®ç‡: {results['accuracy']}%")
            print(f"   âœ“ F1åˆ†æ•°: {results['f1_score']}%")
            print(f"   âœ“ {'âœ“ PASS' if results['pass'] else 'âœ— FAIL'}")
            
        except Exception as e:
            print(f"   âœ— MLæµ‹è¯•å¤±è´¥: {str(e)}")
            self.results['ml_benchmark'] = {'error': str(e)}
    
    def count_pcap_packets(self):
        """ä½¿ç”¨tsharkç»Ÿè®¡PCAPåŒ…æ•°"""
        try:
            cmd = ['tshark', '-r', self.pcap_file, '-q', '-z', 'io,stat,0']
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            # è§£æè¾“å‡º
            for line in result.stdout.split('\n'):
                if 'Frames' in line or 'frames' in line:
                    parts = line.split()
                    for part in parts:
                        if part.isdigit():
                            return int(part)
        except Exception as e:
            print(f"   âš ï¸  æ— æ³•ç»Ÿè®¡PCAPåŒ…æ•°: {e}")
        
        return 0
    
    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report_file = f"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*80)
        print("ğŸ“Š è¯„ä¼°æ‘˜è¦")
        print("="*80)
        
        # ç»Ÿè®¡é€šè¿‡çš„æµ‹è¯•
        passed = 0
        total = 0
        
        for key, value in self.results.items():
            if isinstance(value, dict):
                if 'skipped' in value:
                    continue
                if 'pass' in value:
                    total += 1
                    if value['pass']:
                        passed += 1
                        status = "âœ“ PASS"
                    else:
                        status = "âœ— FAIL"
                    
                    test_name = key.replace('_', ' ').title()
                    print(f"{test_name:30s} {status}")
        
        print("="*80)
        if total > 0:
            print(f"æ€»ä½“é€šè¿‡ç‡: {passed}/{total} ({round(passed/total*100, 1)}%)")
        
        # å…³é”®æŒ‡æ ‡æ€»ç»“
        print("\nå…³é”®æŒ‡æ ‡:")
        
        if 'completeness' in self.results:
            comp = self.results['completeness']
            print(f"  â€¢ æµæ•°é‡: {comp.get('flow_count', 'N/A')}")
            print(f"  â€¢ åŒ…æ•°é‡: {comp.get('total_packets', 'N/A')}")
            if 'coverage_rate' in comp:
                print(f"  â€¢ PCAPè¦†ç›–ç‡: {comp['coverage_rate']}%")
        
        if '5tuple_accuracy' in self.results and 'accuracy' in self.results['5tuple_accuracy']:
            print(f"  â€¢ äº”å…ƒç»„å‡†ç¡®ç‡: {self.results['5tuple_accuracy']['accuracy']}%")
        
        if 'payload_integrity' in self.results and 'match_rate' in self.results['payload_integrity']:
            print(f"  â€¢ PayloadåŒ¹é…ç‡: {self.results['payload_integrity']['match_rate']}%")
        
        if 'ml_benchmark' in self.results and 'accuracy' in self.results['ml_benchmark']:
            print(f"  â€¢ MLå‡†ç¡®ç‡: {self.results['ml_benchmark']['accuracy']}%")
        
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CICIDS2017é¢„å¤„ç†è´¨é‡è¯„ä¼°')
    parser.add_argument('--flows', required=True, help='å•å‘æµPKLæ–‡ä»¶')
    parser.add_argument('--csv', help='ç‰¹å¾CSVæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--pcap', help='åŸå§‹PCAPæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--official', help='å®˜æ–¹CICIDS2017 CSVæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PreprocessingEvaluator(
        flows_pkl=args.flows,
        csv_file=args.csv,
        pcap_file=args.pcap,
        official_csv=args.official
    )
    
    # æ‰§è¡Œè¯„ä¼°
    evaluator.evaluate_all()


if __name__ == '__main__':
    main()
